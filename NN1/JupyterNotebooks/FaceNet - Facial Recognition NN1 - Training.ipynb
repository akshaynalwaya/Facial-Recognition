{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from matplotlib.pyplot import imshow\n",
    "from preprocessing import *\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders_for_training(n_H0, n_W0, n_C0):\n",
    "    # n_H0 -- scalar, height of an input image\n",
    "    # n_W0 -- scalar, width of an input image\n",
    "    # n_C0 -- scalar, number of channels of the input\n",
    "    # Returns\n",
    "    # X,Y,Z -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Z = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    return X, Y, Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(n_C0):\n",
    "    tf.set_random_seed(1)\n",
    "    conv1 = tf.get_variable(\"conv1\", [7,7,n_C0,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2a= tf.get_variable(\"conv2a\", [1,1,64,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2 = tf.get_variable(\"conv2\", [3,3,64,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3a= tf.get_variable(\"conv3a\", [1,1,192,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3 = tf.get_variable(\"conv3\", [3,3,192,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4a= tf.get_variable(\"conv4a\", [1,1,384,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4 = tf.get_variable(\"conv4\", [3,3,384,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5a= tf.get_variable(\"conv5a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5 = tf.get_variable(\"conv5\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6a= tf.get_variable(\"conv6a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6 = tf.get_variable(\"conv6\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc1 = tf.get_variable(\"fc1\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc2   = tf.get_variable(\"fc2\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc7128= tf.get_variable(\"fc7128\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters = {\"conv1\": conv1,\n",
    "                  \"conv2a\": conv2a,\n",
    "                  \"conv2\": conv2,\n",
    "                  \"conv3a\": conv3a,\n",
    "                  \"conv3\": conv3,\n",
    "                  \"conv4a\": conv4a,\n",
    "                  \"conv4\": conv4,\n",
    "                  \"conv5a\": conv5a,\n",
    "                  \"conv5\": conv5,\n",
    "                  \"conv6a\": conv6a,\n",
    "                  \"conv6\": conv6,\n",
    "                  #\"fc1\": fc1,\n",
    "                  #\"fc2\": fc2,\n",
    "                  #\"fc7128\": fc7128,\n",
    "                  }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(parameters,x):\n",
    "    \n",
    "    conv1 = parameters['conv1']\n",
    "    conv2a = parameters['conv2a']\n",
    "    conv2 = parameters['conv2']\n",
    "    conv3a = parameters['conv3a']\n",
    "    conv3 = parameters['conv3']\n",
    "    conv4a = parameters['conv4a']\n",
    "    conv4 = parameters['conv4']\n",
    "    conv5a = parameters['conv5a']\n",
    "    conv5 = parameters['conv5']\n",
    "    conv6a = parameters['conv6a']\n",
    "    conv6 = parameters['conv6']\n",
    "    \n",
    "    #Conv 1\n",
    "    Z1 = tf.nn.conv2d(x,conv1, strides = [1,2,2,1], padding = 'VALID')\n",
    "    with tf.variable_scope(\"b1\") as scope:\n",
    "        B1 = tf.layers.batch_normalization(Z1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A1 = tf.nn.relu(B1)\n",
    "        P1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        N1 = tf.nn.lrn(P1)\n",
    "    \n",
    "    #Conv 2A\n",
    "    Z2a = tf.nn.conv2d(N1,conv2a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2a = tf.nn.relu(Z2a)\n",
    "   \n",
    "    #Conv 2\n",
    "    Z2 = tf.nn.conv2d(A2a,conv2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b2\") as scope:\n",
    "        B2 = tf.layers.batch_normalization(Z2, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A2 = tf.nn.relu(B2)\n",
    "        P2 = tf.nn.max_pool(A2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        N2 = tf.nn.lrn(P2)\n",
    "     \n",
    "    #Conv 3A\n",
    "    Z3a = tf.nn.conv2d(N2,conv3a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A3a = tf.nn.relu(Z3a)  \n",
    "    \n",
    "    #Conv 3\n",
    "    Z3 = tf.nn.conv2d(A3a,conv3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b3\") as scope:\n",
    "        B3 = tf.layers.batch_normalization(Z3, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A3 = tf.nn.relu(B3)\n",
    "        P3 = tf.nn.max_pool(A3, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        N3 = tf.nn.lrn(P3)\n",
    "    \n",
    "    #Conv 4a\n",
    "    Z4a = tf.nn.conv2d(N3,conv4a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A4a = tf.nn.relu(Z4a)\n",
    "    \n",
    "    #Conv 4\n",
    "    Z4 = tf.nn.conv2d(A4a,conv4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b4\") as scope:\n",
    "        B4 = tf.layers.batch_normalization(Z4, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A4 = tf.nn.relu(B4)\n",
    "        N4 = tf.nn.lrn(A4)\n",
    "    \n",
    "    #Conv 5a\n",
    "    Z5a = tf.nn.conv2d(N4,conv5a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5a = tf.nn.relu(Z5a)\n",
    "    \n",
    "    #Conv 5\n",
    "    Z5 = tf.nn.conv2d(A5a,conv5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b5\") as scope:\n",
    "        B5 = tf.layers.batch_normalization(Z5, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A5 = tf.nn.relu(B5)\n",
    "        N5 = tf.nn.lrn(A5)\n",
    "    \n",
    "    #Conv 6a\n",
    "    Z6a = tf.nn.conv2d(N5,conv6a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A6a = tf.nn.relu(Z6a)\n",
    "    \n",
    "    #Conv 6\n",
    "    Z6 = tf.nn.conv2d(A6a,conv6, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b6\") as scope:\n",
    "        B6 = tf.layers.batch_normalization(Z6, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A6 = tf.nn.relu(B6)\n",
    "        P6 = tf.nn.max_pool(A6, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #Flattening\n",
    "    P6F = tf.contrib.layers.flatten(P6)\n",
    "    \n",
    "    #FC 1\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        Z_FC1 = tf.contrib.layers.fully_connected(P6F,32*256,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC1 = tf.nn.relu(Z_FC1)\n",
    "    #Maxout\n",
    "    #M_FC1 = tf.contrib.layers.maxout(A_FC1,32*128)\n",
    "    \n",
    "    #FC_2\n",
    "    with tf.variable_scope(\"fc2\") as scope:\n",
    "        Z_FC2 = tf.contrib.layers.fully_connected(A_FC1,32*256,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC2 = tf.nn.relu(Z_FC2)\n",
    "\n",
    "    #Maxout\n",
    "    #M_FC2 = tf.contrib.layers.maxout(A_FC2,32*128)\n",
    "    \n",
    "    #FC_7128\n",
    "    with tf.variable_scope(\"fc3\") as scope:\n",
    "        Z_FC7 = tf.contrib.layers.fully_connected(A_FC2,128,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC7 = tf.nn.relu(Z_FC7)\n",
    "\n",
    "    #l2 Normalization\n",
    "    embeddings = tf.nn.l2_normalize(A_FC7)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss_debug(y_pred, alpha = 0.5):\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "   \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "    \n",
    "    pos_dist2 = tf.Print(pos_dist, [pos_dist], \"pos_dist \")\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    \n",
    "    neg_dist2 = tf.Print(neg_dist, [neg_dist], \"neg_dist \")\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist2, neg_dist2) , alpha)\n",
    "    basic_loss2 = tf.Print(basic_loss, [basic_loss], \"basic loss: \")\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss2,0.0))\n",
    "    \n",
    "    loss2 = tf.Print(loss, [loss], \"loss \")\n",
    "    \n",
    "    return loss2\n",
    "\n",
    "def triplet_loss(y_pred, alpha = 0.5):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis = -1)\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis = -1)\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist) , alpha)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSampleData():\n",
    "    fname = \"./SampleDataset/1a.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    anchor = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1b.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    positive = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1c.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    \n",
    "    negative = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    #print(negative.shape)\n",
    "    \n",
    "    return anchor,positive,negative\n",
    "    #return 0,0,0\n",
    "\n",
    "#anchor,positive,negative = loadSampleData()\n",
    "def getLoaderInstance(batchSize = 64, batches = 15):\n",
    "    loaderInstance = DataReader(batchSize,batches)\n",
    "    return loaderInstance\n",
    "\n",
    "def loadDataBatch(loaderInstance):\n",
    "    tempList = loaderInstance.getData()\n",
    "    anchor, positive, negative = (np.array(tempList[0]),np.array(tempList[1]),np.array(tempList[2]))\n",
    "    #print(anchor.shape)\n",
    "    return anchor,positive,negative\n",
    "\n",
    "#anchor,positive,negative = loadDataBatch()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Loader to start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3113424e2a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataLoaderInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLoaderInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-3d425aebd8c3>\u001b[0m in \u001b[0;36mgetLoaderInstance\u001b[1;34m(batchSize, batches)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#anchor,positive,negative = loadSampleData()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetLoaderInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloaderInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloaderInstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataReader' is not defined"
     ]
    }
   ],
   "source": [
    "batchSize = 64\n",
    "batches = 40\n",
    "dataLoaderInstance = getLoaderInstance(batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "(64, 220, 220, 3)\n"
     ]
    }
   ],
   "source": [
    "anchor = np.empty((batchSize,220,220,3))\n",
    "positive = np.empty((batchSize,220,220,3))\n",
    "negative = np.empty((batchSize,220,220,3))\n",
    "for i in range(batches):\n",
    "    anchor,positive,negative = loadDataBatch(dataLoaderInstance)\n",
    "    with open('./cache/inputs'+str(i)+'.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([anchor, positive, negative], f, 0)\n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "        \n",
    "print(anchor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching the loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('inputs6415.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([anchor, positive, negative], f)\n",
    "# imshow(positive[1])\n",
    "# import cv2\n",
    "# cv2.imwrite(\"image.jpg\", negative[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 220, 220, 3)\n"
     ]
    }
   ],
   "source": [
    "with open('../inputs6415.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    anchor,positive,negative = pickle.load(f)\n",
    "print(anchor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadCache(iter):\n",
    "    with open('./cache/inputs'+str(iter)+'.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        anchor,positive,negative = pickle.load(f)\n",
    "    return anchor,positive,negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"FaceNet\", reuse=tf.AUTO_REUSE):\n",
    "    x,y,z = create_placeholders_for_training(220,220,3)\n",
    "    params = init_params(3)\n",
    "    preds1 = forward_prop(params,x)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    preds2 = forward_prop(params,y)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    preds3 = forward_prop(params,z)\n",
    "\n",
    "loss = triplet_loss([preds1,preds2,preds3],0.5)\n",
    "optim = tf.train.AdamOptimizer(0.00001,name = 'optim').minimize(loss)\n",
    "\n",
    "init  = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./faceNet32\n",
      "Restoring Model\n",
      "i:0, loss = 1.50103\n",
      "i:2, loss = 1.51566\n",
      "i:4, loss = 3.02656\n",
      "i:6, loss = 2.85161\n",
      "Avg Loss :2.21533884321\n",
      "i:0, loss = 2.1374\n",
      "i:2, loss = 3.35984\n",
      "i:4, loss = 3.48242\n",
      "i:6, loss = 3.49519\n",
      "Avg Loss :3.26832294464\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 2\n",
    "    restore = True\n",
    "    save = False\n",
    "    #saver.restore(sess, './faceNet3')\n",
    "    train_cache_file = './faceNet32.meta'\n",
    "    if os.path.exists(train_cache_file) and restore:\n",
    "        saver.restore(sess, './faceNet32')\n",
    "        print(\"Restoring Model\")\n",
    "    else:\n",
    "        print(\"No Saved Model Found. Starting training from scratch.\")\n",
    "    for epoch in range(epochs):\n",
    "        avgCost = 0\n",
    "        iters = 7\n",
    "        for i in range(iters):\n",
    "            #anchor,positive,negative = loadCache(i)\n",
    "            curr_cost, _  = sess.run([loss, optim],feed_dict = {x:anchor[i*8:i*8+7],y:positive[i*8:i*8+7],z:negative[i*8:i*8+7]})\n",
    "            avgCost+=curr_cost\n",
    "            if(i%2==0):\n",
    "                print(\"i:\"+str(i)+\", loss = \"+str(curr_cost))\n",
    "            #print(embed1)\n",
    "            #print(embed2)\n",
    "        avgCost/=iters\n",
    "        print(\"Avg Loss :\"+str(avgCost))\n",
    "    if save:\n",
    "        saver.save(sess, './faceNet32')\n",
    "        print(\"Model Saved to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDistanceBetweenEmbeddings(embedding1, embedding2):\n",
    "    dist = np.sum(np.square(np.subtract(embedding1,embedding2)))\n",
    "    return dist\n",
    "\n",
    "# anchor = tf.placeholder(tf.float32, shape=(None,220,220,3))\n",
    "# positive = tf.placeholder(tf.float32, shape=(None,220,220,3))\n",
    "# pos_dist2 = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "\n",
    "emb1 = sess.run(preds1,feed_dict = {x:np.expand_dims(anchor[16],axis =0)})\n",
    "emb2 = sess.run(preds1,feed_dict = {x:np.expand_dims(negative[16],axis =0)})\n",
    "\n",
    "#tfdist = sess.run(pos_dist2,feed_dict = {anchor:np.expand_dims(anchor[16],axis =0),positive:np.expand_dims(positive[16],axis =0)})\n",
    "\n",
    "#emb3 = sess.run(preds1,feed_dict = {x:np.expand_dims(anchor[16],axis =0)})\n",
    "emb4 = sess.run(preds1,feed_dict = {x:np.expand_dims(positive[16],axis =0)})\n",
    "\n",
    "print(emb1)\n",
    "print(emb4)\n",
    "dist = getDistanceBetweenEmbeddings(emb1,emb2)\n",
    "print(dist)\n",
    "dist2 = getDistanceBetweenEmbeddings(emb1,emb4)\n",
    "print(dist2)\n",
    "#print(tfdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my-firstmodel1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
