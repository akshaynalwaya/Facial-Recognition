{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from matplotlib.pyplot import imshow\n",
    "from preprocessing import *\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders_for_training(n_H0, n_W0, n_C0):\n",
    "    # n_H0 -- scalar, height of an input image\n",
    "    # n_W0 -- scalar, width of an input image\n",
    "    # n_C0 -- scalar, number of channels of the input\n",
    "    # Returns\n",
    "    # X,Y,Z -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Z = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    return X, Y, Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(n_C0):\n",
    "    tf.set_random_seed(1)\n",
    "    conv1 = tf.get_variable(\"conv1\", [7,7,n_C0,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2a= tf.get_variable(\"conv2a\", [1,1,64,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2 = tf.get_variable(\"conv2\", [3,3,64,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3a= tf.get_variable(\"conv3a\", [1,1,192,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3 = tf.get_variable(\"conv3\", [3,3,192,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4a= tf.get_variable(\"conv4a\", [1,1,384,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4 = tf.get_variable(\"conv4\", [3,3,384,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5a= tf.get_variable(\"conv5a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5 = tf.get_variable(\"conv5\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6a= tf.get_variable(\"conv6a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6 = tf.get_variable(\"conv6\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc1 = tf.get_variable(\"fc1\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc2   = tf.get_variable(\"fc2\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc7128= tf.get_variable(\"fc7128\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters = {\"conv1\": conv1,\n",
    "                  \"conv2a\": conv2a,\n",
    "                  \"conv2\": conv2,\n",
    "                  \"conv3a\": conv3a,\n",
    "                  \"conv3\": conv3,\n",
    "                  \"conv4a\": conv4a,\n",
    "                  \"conv4\": conv4,\n",
    "                  \"conv5a\": conv5a,\n",
    "                  \"conv5\": conv5,\n",
    "                  \"conv6a\": conv6a,\n",
    "                  \"conv6\": conv6,\n",
    "                  #\"fc1\": fc1,\n",
    "                  #\"fc2\": fc2,\n",
    "                  #\"fc7128\": fc7128,\n",
    "                  }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(parameters,x):\n",
    "    \n",
    "    conv1 = parameters['conv1']\n",
    "    conv2a = parameters['conv2a']\n",
    "    conv2 = parameters['conv2']\n",
    "    conv3a = parameters['conv3a']\n",
    "    conv3 = parameters['conv3']\n",
    "    conv4a = parameters['conv4a']\n",
    "    conv4 = parameters['conv4']\n",
    "    conv5a = parameters['conv5a']\n",
    "    conv5 = parameters['conv5']\n",
    "    conv6a = parameters['conv6a']\n",
    "    conv6 = parameters['conv6']\n",
    "    \n",
    "    #Conv 1\n",
    "    Z1 = tf.nn.conv2d(x,conv1, strides = [1,2,2,1], padding = 'VALID')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    N1 = tf.nn.lrn(P1)\n",
    "    \n",
    "    #Conv 2A\n",
    "    Z2a = tf.nn.conv2d(N1,conv2a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2a = tf.nn.relu(Z2a)\n",
    "   \n",
    "    #Conv 2\n",
    "    Z2 = tf.nn.conv2d(A2a,conv2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    N2 = tf.nn.lrn(A2)\n",
    "    P2 = tf.nn.max_pool(N2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "     \n",
    "    #Conv 3A\n",
    "    Z3a = tf.nn.conv2d(P2,conv3a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A3a = tf.nn.relu(Z3a)  \n",
    "    \n",
    "    #Conv 3\n",
    "    Z3 = tf.nn.conv2d(A3a,conv3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    P3 = tf.nn.max_pool(A3, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #Conv 4a\n",
    "    Z4a = tf.nn.conv2d(P3,conv4a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A4a = tf.nn.relu(Z4a)\n",
    "    \n",
    "    #Conv 4\n",
    "    Z4 = tf.nn.conv2d(A4a,conv4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    \n",
    "    #Conv 5a\n",
    "    Z5a = tf.nn.conv2d(A4,conv5a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5a = tf.nn.relu(Z5a)\n",
    "    \n",
    "    #Conv 5\n",
    "    Z5 = tf.nn.conv2d(A5a,conv5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    \n",
    "    #Conv 6a\n",
    "    Z6a = tf.nn.conv2d(A5,conv6a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A6a = tf.nn.relu(Z6a)\n",
    "    \n",
    "    #Conv 6\n",
    "    Z6 = tf.nn.conv2d(A6a,conv6, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A6 = tf.nn.relu(Z6)\n",
    "    P6 = tf.nn.max_pool(A6, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #Flattening\n",
    "    P6F = tf.contrib.layers.flatten(P6)\n",
    "    \n",
    "    #FC 1\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        Z_FC1 = tf.contrib.layers.fully_connected(P6F,32*256,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC1 = tf.nn.relu(Z_FC1)\n",
    "    #Maxout\n",
    "    #M_FC1 = tf.contrib.layers.maxout(A_FC1,32*128)\n",
    "    \n",
    "    #FC_2\n",
    "    with tf.variable_scope(\"fc2\") as scope:\n",
    "        Z_FC2 = tf.contrib.layers.fully_connected(A_FC1,32*256,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC2 = tf.nn.relu(Z_FC2)\n",
    "\n",
    "    #Maxout\n",
    "    #M_FC2 = tf.contrib.layers.maxout(A_FC2,32*128)\n",
    "    \n",
    "    #FC_7128\n",
    "    with tf.variable_scope(\"fc3\") as scope:\n",
    "        Z_FC7 = tf.contrib.layers.fully_connected(A_FC2,128,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC7 = tf.nn.relu(Z_FC7)\n",
    "\n",
    "    #l2 Normalization\n",
    "    embeddings = tf.nn.l2_normalize(A_FC7)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss_debug(y_pred, alpha = 0.5):\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "   \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "    \n",
    "    pos_dist2 = tf.Print(pos_dist, [pos_dist], \"pos_dist \")\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    \n",
    "    neg_dist2 = tf.Print(neg_dist, [neg_dist], \"neg_dist \")\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist2, neg_dist2) , alpha)\n",
    "    basic_loss2 = tf.Print(basic_loss, [basic_loss], \"basic loss: \")\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss2,0.0))\n",
    "    \n",
    "    loss2 = tf.Print(loss, [loss], \"loss \")\n",
    "    \n",
    "    return loss2\n",
    "\n",
    "def triplet_loss(y_pred, alpha = 0.5):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist) , alpha)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSampleData():\n",
    "    fname = \"./SampleDataset/1a.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    anchor = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1b.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    positive = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1c.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    \n",
    "    negative = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    #print(negative.shape)\n",
    "    \n",
    "    return anchor,positive,negative\n",
    "    #return 0,0,0\n",
    "\n",
    "#anchor,positive,negative = loadSampleData()\n",
    "def getLoaderInstance(batchSize = 64, batches = 15):\n",
    "    loaderInstance = DataReader(batchSize,batches)\n",
    "    return loaderInstance\n",
    "\n",
    "def loadDataBatch(loaderInstance):\n",
    "    tempList = loaderInstance.getData()\n",
    "    anchor, positive, negative = (np.array(tempList[0]),np.array(tempList[1]),np.array(tempList[2]))\n",
    "    #print(anchor.shape)\n",
    "    return anchor,positive,negative\n",
    "\n",
    "#anchor,positive,negative = loadDataBatch()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Loader to start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3113424e2a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataLoaderInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLoaderInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-3d425aebd8c3>\u001b[0m in \u001b[0;36mgetLoaderInstance\u001b[1;34m(batchSize, batches)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#anchor,positive,negative = loadSampleData()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetLoaderInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloaderInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloaderInstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataReader' is not defined"
     ]
    }
   ],
   "source": [
    "batchSize = 64\n",
    "batches = 40\n",
    "dataLoaderInstance = getLoaderInstance(batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "(64, 220, 220, 3)\n"
     ]
    }
   ],
   "source": [
    "anchor = np.empty((batchSize,220,220,3))\n",
    "positive = np.empty((batchSize,220,220,3))\n",
    "negative = np.empty((batchSize,220,220,3))\n",
    "for i in range(batches):\n",
    "    anchor,positive,negative = loadDataBatch(dataLoaderInstance)\n",
    "    with open('./cache/inputs'+str(i)+'.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([anchor, positive, negative], f, 0)\n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "        \n",
    "print(anchor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching the loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('inputs6415.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([anchor, positive, negative], f)\n",
    "# imshow(positive[1])\n",
    "# import cv2\n",
    "# cv2.imwrite(\"image.jpg\", negative[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('inputs6415.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#         anchor,positive,negative = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadCache(iter):\n",
    "    with open('./cache/inputs'+str(iter)+'.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        anchor,positive,negative = pickle.load(f)\n",
    "    return anchor,positive,negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\maxout.py:109: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to `Dense` should be defined. Found `None`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bb3c90b106b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_placeholders_for_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m220\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m220\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpreds1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpreds2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-be5a01d37d4d>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(parameters, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m#FC_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fc2\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mZ_FC2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM_FC1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreuse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mA_FC2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ_FC2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\u001b[0m in \u001b[0;36mfully_connected\u001b[1;34m(inputs, num_outputs, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[0;32m   1716\u001b[0m         \u001b[0m_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1717\u001b[0m         _reuse=reuse)\n\u001b[1;32m-> 1718\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[1;31m# Add variables to collections.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \"\"\"\n\u001b[1;32m--> 809\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m               \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m           \u001b[1;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m       raise ValueError('The last dimension of the inputs to `Dense` '\n\u001b[0m\u001b[0;32m    125\u001b[0m                        'should be defined. Found `None`.')\n\u001b[0;32m    126\u001b[0m     self.input_spec = base.InputSpec(min_ndim=2,\n",
      "\u001b[1;31mValueError\u001b[0m: The last dimension of the inputs to `Dense` should be defined. Found `None`."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"FaceNet\", reuse=tf.AUTO_REUSE):\n",
    "    x,y,z = create_placeholders_for_training(220,220,3)\n",
    "    params = init_params(3)\n",
    "    preds1 = forward_prop(params,x)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    preds2 = forward_prop(params,y)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    preds3 = forward_prop(params,z)\n",
    "\n",
    "loss = triplet_loss([preds1,preds2,preds3],0.2)\n",
    "optim = tf.train.AdagradOptimizer(0.05,name = 'optim').minimize(loss)\n",
    "\n",
    "init  = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./faceNet1\n",
      "i:0, loss = 0.199923\n",
      "i:2, loss = 0.199934\n",
      "i:4, loss = 0.199881\n",
      "i:6, loss = 0.199769\n",
      "i:8, loss = 0.199919\n",
      "i:10, loss = 0.199949\n",
      "i:12, loss = 0.199678\n",
      "i:14, loss = 0.199781\n",
      "i:16, loss = 0.199769\n",
      "i:18, loss = 0.199461\n",
      "i:20, loss = 0.199849\n",
      "i:22, loss = 0.19995\n",
      "i:24, loss = 0.199568\n",
      "i:26, loss = 0.199817\n",
      "i:28, loss = 0.199699\n",
      "i:30, loss = 0.199335\n",
      "i:32, loss = 0.199408\n",
      "i:34, loss = 0.199316\n",
      "i:36, loss = 0.198789\n",
      "i:38, loss = 0.199601\n",
      "Avg Loss :0.199711428955\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 1\n",
    "    saver.restore(sess, './faceNet1')\n",
    "    for epoch in range(epochs):\n",
    "        avgCost = 0\n",
    "        iters = batches\n",
    "        for i in range(iters):\n",
    "            anchor,positive,negative = loadCache(i)\n",
    "            curr_cost, _  = sess.run([loss, optim],feed_dict = {x:anchor,y:positive,z:negative})\n",
    "            avgCost+=curr_cost\n",
    "            if(i%2==0):\n",
    "                print(\"i:\"+str(i)+\", loss = \"+str(curr_cost))\n",
    "            #print(embed1)\n",
    "            #print(embed2)\n",
    "        avgCost/=iters\n",
    "        print(\"Avg Loss :\"+str(avgCost))\n",
    "    \n",
    "    saver.save(sess, './faceNet1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDistanceBetweenEmbeddings(embedding1, embedding2):\n",
    "    dist = np.sum(np.square(np.subtract(embedding1,embedding2)))\n",
    "    return dist\n",
    "\n",
    "# anchor = tf.placeholder(tf.float32, shape=(None,220,220,3))\n",
    "# positive = tf.placeholder(tf.float32, shape=(None,220,220,3))\n",
    "# pos_dist2 = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "\n",
    "emb1 = sess.run(preds1,feed_dict = {x:np.expand_dims(anchor[16],axis =0)})\n",
    "emb2 = sess.run(preds1,feed_dict = {x:np.expand_dims(negative[16],axis =0)})\n",
    "\n",
    "#tfdist = sess.run(pos_dist2,feed_dict = {anchor:np.expand_dims(anchor[16],axis =0),positive:np.expand_dims(positive[16],axis =0)})\n",
    "\n",
    "#emb3 = sess.run(preds1,feed_dict = {x:np.expand_dims(anchor[16],axis =0)})\n",
    "emb4 = sess.run(preds1,feed_dict = {x:np.expand_dims(positive[16],axis =0)})\n",
    "\n",
    "print(emb1)\n",
    "print(emb4)\n",
    "dist = getDistanceBetweenEmbeddings(emb1,emb2)\n",
    "print(dist)\n",
    "dist2 = getDistanceBetweenEmbeddings(emb1,emb4)\n",
    "print(dist2)\n",
    "#print(tfdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my-firstmodel1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
