{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders_for_training(n_H0, n_W0, n_C0):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Z = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    return X, Y, Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(n_C0):\n",
    "    tf.set_random_seed(1)\n",
    "    conv1 = tf.get_variable(\"conv1\", [7,7,n_C0,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2a= tf.get_variable(\"conv2a\", [1,1,64,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2 = tf.get_variable(\"conv2\", [3,3,64,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3a= tf.get_variable(\"conv3a\", [1,1,192,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3 = tf.get_variable(\"conv3\", [3,3,192,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4a= tf.get_variable(\"conv4a\", [1,1,384,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4 = tf.get_variable(\"conv4\", [3,3,384,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5a= tf.get_variable(\"conv5a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5 = tf.get_variable(\"conv5\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6a= tf.get_variable(\"conv6a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6 = tf.get_variable(\"conv6\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc1 = tf.get_variable(\"fc1\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc2   = tf.get_variable(\"fc2\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc7128= tf.get_variable(\"fc7128\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters = {\"conv1\": conv1,\n",
    "                  \"conv2a\": conv2a,\n",
    "                  \"conv2\": conv2,\n",
    "                  \"conv3a\": conv3a,\n",
    "                  \"conv3\": conv3,\n",
    "                  \"conv4a\": conv4a,\n",
    "                  \"conv4\": conv4,\n",
    "                  \"conv5a\": conv5a,\n",
    "                  \"conv5\": conv5,\n",
    "                  \"conv6a\": conv6a,\n",
    "                  \"conv6\": conv6,\n",
    "                  #\"fc1\": fc1,\n",
    "                  #\"fc2\": fc2,\n",
    "                  #\"fc7128\": fc7128,\n",
    "                  }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(parameters,x):\n",
    "    \n",
    "    conv1 = parameters['conv1']\n",
    "    conv2a = parameters['conv2a']\n",
    "    conv2 = parameters['conv2']\n",
    "    conv3a = parameters['conv3a']\n",
    "    conv3 = parameters['conv3']\n",
    "    conv4a = parameters['conv4a']\n",
    "    conv4 = parameters['conv4']\n",
    "    conv5a = parameters['conv5a']\n",
    "    conv5 = parameters['conv5']\n",
    "    conv6a = parameters['conv6a']\n",
    "    conv6 = parameters['conv6']\n",
    "    \n",
    "    #Conv 1\n",
    "    Z1 = tf.nn.conv2d(x,conv1, strides = [1,2,2,1], padding = 'VALID')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    N1 = tf.nn.lrn(P1)\n",
    "    \n",
    "    #Conv 2A\n",
    "    Z2a = tf.nn.conv2d(N1,conv2a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2a = tf.nn.relu(Z2a)\n",
    "   \n",
    "    #Conv 2\n",
    "    Z2 = tf.nn.conv2d(A2a,conv2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    N2 = tf.nn.lrn(A2)\n",
    "    P2 = tf.nn.max_pool(N2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "     \n",
    "    #Conv 3A\n",
    "    Z3a = tf.nn.conv2d(P2,conv3a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A3a = tf.nn.relu(Z3a)  \n",
    "    \n",
    "    #Conv 3\n",
    "    Z3 = tf.nn.conv2d(A3a,conv3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    P3 = tf.nn.max_pool(A3, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #Conv 4a\n",
    "    Z4a = tf.nn.conv2d(P3,conv4a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A4a = tf.nn.relu(Z4a)\n",
    "    \n",
    "    #Conv 4\n",
    "    Z4 = tf.nn.conv2d(A4a,conv4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    \n",
    "    #Conv 5a\n",
    "    Z5a = tf.nn.conv2d(A4,conv5a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5a = tf.nn.relu(Z5a)\n",
    "    \n",
    "    #Conv 5\n",
    "    Z5 = tf.nn.conv2d(A5a,conv5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    \n",
    "    #Conv 6a\n",
    "    Z6a = tf.nn.conv2d(A5,conv6a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A6a = tf.nn.relu(Z6a)\n",
    "    \n",
    "    #Conv 6\n",
    "    Z6 = tf.nn.conv2d(A6a,conv6, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A6 = tf.nn.relu(Z6)\n",
    "    P6 = tf.nn.max_pool(A6, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #Flattening\n",
    "    P6F = tf.contrib.layers.flatten(P6)\n",
    "    \n",
    "    #FC 1\n",
    "    Z_FC1 = tf.contrib.layers.fully_connected(P6F,32*256,activation_fn=None)\n",
    "    A_FC1 = tf.nn.relu(Z_FC1)\n",
    "    #Maxout\n",
    "    #M_FC1 = tf.contrib.layers.maxout(A_FC1,32*128)\n",
    "    \n",
    "    #FC_2\n",
    "    Z_FC2 = tf.contrib.layers.fully_connected(A_FC1,32*256,activation_fn=None)\n",
    "    A_FC2 = tf.nn.relu(Z_FC2)\n",
    "\n",
    "    #Maxout\n",
    "    #M_FC2 = tf.contrib.layers.maxout(A_FC2,32*128)\n",
    "    \n",
    "    #FC_7128\n",
    "    Z_FC7 = tf.contrib.layers.fully_connected(A_FC2,128,activation_fn=None)\n",
    "    A_FC7 = tf.nn.relu(Z_FC7)\n",
    "\n",
    "    #l2 Normalization\n",
    "    embeddings = tf.nn.l2_normalize(A_FC7)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss_debug(y_pred, alpha = 0.5):\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "   \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "    \n",
    "    pos_dist2 = tf.Print(pos_dist, [pos_dist], \"pos_dist \")\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    \n",
    "    neg_dist2 = tf.Print(neg_dist, [neg_dist], \"neg_dist \")\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist2, neg_dist2) , alpha)\n",
    "    basic_loss2 = tf.Print(basic_loss, [basic_loss], \"basic loss: \")\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss2,0.0))\n",
    "    \n",
    "    loss2 = tf.Print(loss, [loss], \"loss \")\n",
    "    \n",
    "    return loss2\n",
    "\n",
    "def triplet_loss(y_pred, alpha = 0.5):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist) , alpha)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    fname = \"./SampleDataset/1a.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    anchor = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1b.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    positive = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1c.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    \n",
    "    negative = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    #print(negative.shape)\n",
    "    \n",
    "    return anchor,positive,negative\n",
    "    #return 0,0,0\n",
    "\n",
    "anchor,positive,negative = loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x,y,z = create_placeholders_for_training(220,220,3)\n",
    "params = init_params(3)\n",
    "preds1 = forward_prop(params,x)\n",
    "preds2 = forward_prop(params,y)\n",
    "preds3 = forward_prop(params,z)\n",
    "loss = triplet_loss_debug([preds1,preds2,preds3])\n",
    "optim = tf.train.GradientDescentOptimizer(0.01, name = 'optim').minimize(loss)\n",
    "\n",
    "\n",
    "init  = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.232676\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "sess.run(init)\n",
    "for i in range(3):\n",
    "    print(loss.eval(feed_dict = {x:anchor,y:positive,z:negative}))\n",
    "    sess.run(optim,feed_dict = {x:anchor,y:positive,z:negative})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-51d6e2890b07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./my-firstmodel1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1652\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[0;32m   1653\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1654\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1655\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m           self._build_eager(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, './my-firstmodel1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
